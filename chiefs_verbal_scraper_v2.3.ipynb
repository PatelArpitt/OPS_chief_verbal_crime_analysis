{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pre-requisite libraries for the project\n",
    "\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import re\n",
    "from itertools import cycle\n",
    "import traceback\n",
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.application import MIMEApplication\n",
    "import os\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import PyPDF2, io\n",
    "import numpy as np\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# Setting up environment variables\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate Selenium Framework (Chromium Headless Browser)\n",
    "\n",
    "def initialize_chromium():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--disable-infobars\")\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    options.add_argument(\"--disable-extensions\")\n",
    "    # options.add_argument('--window-size=764,1080')\n",
    "    options.add_argument('--window-size=1920,1080')\n",
    "    # options.add_argument(\"--headless\")\n",
    "    global driver; driver = webdriver.Chrome(options = options)\n",
    "\n",
    "\n",
    "# Release Selenium Framework (Chromium Headless Browser)\n",
    "\n",
    "def release_chromium():\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the Regular Meetings data to a DataFrame\n",
    "\n",
    "def meetings_extraction():\n",
    "  # url = 'https://ottawapoliceboard.ca/opsb-cspo/meetings.html'\n",
    "  # url = 'https://pub-ottawa.escribemeetings.com/?Expanded=Ottawa%20Police%20Services%20Board&Year=2023'\n",
    "  # url = 'https://pub-ottawa.escribemeetings.com/Meeting.aspx?Id=3760766e-baa1-4574-a3c3-da5e7879c8c6&Agenda=Agenda&lang=English'\n",
    "\n",
    "  global current_year; current_year = 2022\n",
    "  meeting_no = 1\n",
    "  reg_meets_all = []\n",
    "  reg_meets = []\n",
    "  reg_meets_cancelled = []\n",
    "  reg_meets_without_verbals = []\n",
    "\n",
    "  while current_year > 2010:\n",
    "    url = f'https://ottawapoliceboard.ca/opsb-cspo/{current_year}-meetings.html'\n",
    "\n",
    "    driver.get(url)\n",
    "    page_src = driver.page_source\n",
    "    soup = BeautifulSoup(page_src, 'html.parser')\n",
    "\n",
    "    meetings_list = soup.find('table').find('tbody').find_all('tr')\n",
    "\n",
    "    for index, tr in enumerate(reversed(meetings_list)):\n",
    "        td = tr.find_all('td')\n",
    "        if td[3].get_text().strip() == 'Regular Meeting':\n",
    "          if current_year < 2013: date = f'{td[0].get_text().strip()}, {current_year}'\n",
    "          else: date = td[0].get_text().strip()\n",
    "\n",
    "          # driver.get(td[0].find('a').get('href'))\n",
    "          # page_src = driver.page_source\n",
    "\n",
    "          reg_meets_all.append({\n",
    "              'Meeting #': meeting_no,\n",
    "              'Date': date,\n",
    "              'Location': td[1].get_text().strip(),\n",
    "              'Time': td[2].get_text().strip(),\n",
    "              'Meeting Type': td[3].get_text().strip(),\n",
    "              'Meeting Page': td[0].find('a').get('href'),\n",
    "              'Chief Verbal Report Present': '',\n",
    "              'Verbal Report File URL': ''\n",
    "            })\n",
    "          meeting_no = meeting_no + 1\n",
    "    print(f'Regular Meetings of Year {current_year} processed.')\n",
    "    current_year = current_year - 1\n",
    "  reg_meets_all_df = pd.DataFrame(reg_meets_all)\n",
    "\n",
    "# Remove Cancelled Regular Meetings\n",
    "# soup.find(lambda tag: tag.name == 'div' and 'I CAN NOT GET THIS' in tag.get_text())\n",
    "  for x in reg_meets_all:\n",
    "    if x['Location'] == 'CANCELLED':\n",
    "      reg_meets_cancelled.append(x)\n",
    "    else:\n",
    "      reg_meets.append(x)\n",
    "  reg_meets_cancelled_df = pd.DataFrame(reg_meets_cancelled)\n",
    "  # reg_meets_df = pd.DataFrame(reg_meets)\n",
    "\n",
    "  for meet in reg_meets:\n",
    "    # initialize_chromium()\n",
    "\n",
    "    url = meet['Meeting Page']\n",
    "    domain = '{uri.scheme}://{uri.netloc}/'.format(uri = urlparse(url))\n",
    "    driver.get(url)\n",
    "    page_src = driver.page_source\n",
    "    soup = BeautifulSoup(page_src, 'html.parser')\n",
    "\n",
    "\n",
    "    elems = soup.find_all(text = lambda t: t and any(x in t for x in [\"Chief’s verbal report\", \"Chief's verbal report\", \"CHIEF’S VERBAL REPORT\", \"CHIEF'S VERBAL REPORT\"]))\n",
    "    for el in elems:\n",
    "      if ((el.find_parent()).find_parent()).find_parent().find_all('a', href = lambda href: href and \"filestream\" in href):\n",
    "        urls = ((el.find_parent()).find_parent()).find_parent().find_all('a', href = lambda href: href and \"filestream\" in href)\n",
    "        if url:\n",
    "          for a in urls:\n",
    "            # if \"filestream\" in a['href']:\n",
    "              verbal_report_url = f\"{domain}{a['href']}\"\n",
    "              meet.update({'Chief Verbal Report Present': 'Yes'})\n",
    "              meet.update({'Verbal Report File URL': verbal_report_url})\n",
    "        else:\n",
    "          meet.update({'Chief Verbal Report Present': 'No'})\n",
    "          meet.update({'Verbal Report File URL': '-'})\n",
    "      elif (((el.find_parent()).find_parent()).find_parent()).find_parent().find_all('a', href = lambda href: href and \"filestream\" in href):\n",
    "        urls = (((el.find_parent()).find_parent()).find_parent()).find_parent().find_all('a', href = lambda href: href and \"filestream\" in href)\n",
    "        if urls:\n",
    "          for a in urls:\n",
    "            if \"filestream\" in a['href']:\n",
    "              verbal_report_url = f\"{domain}{a['href']}\"\n",
    "              meet.update({'Chief Verbal Report Present': 'Yes'})\n",
    "              meet.update({'Verbal Report File URL': verbal_report_url})\n",
    "            else:\n",
    "              meet.update({'Chief Verbal Report Present': 'No'})\n",
    "              meet.update({'Verbal Report File URL': '-'})\n",
    "        else:\n",
    "          meet.update({'Chief Verbal Report Present': 'No'})\n",
    "          meet.update({'Verbal Report File URL': '-'})\n",
    "\n",
    "\n",
    "  reg_meets_df = pd.DataFrame(reg_meets)\n",
    "  export_to_excel(reg_meets_df)\n",
    "\n",
    "\n",
    "# # Check for Police Chief Verbals from the meetings list\n",
    "#   for x in reg_meets:\n",
    "#     url = f\"{x['Meeting Page']}\"\n",
    "#     driver.get(url)\n",
    "#     page_src = driver.page_source\n",
    "#     soup = BeautifulSoup(page_src, 'html.parser')\n",
    "\n",
    "    # chief_verbal_attachment = soup.find('table').find('tbody').find_all('tr')\n",
    "\n",
    "# Export the DataFrame to an Excel Sheet\n",
    "\n",
    "def export_to_excel(df):\n",
    "  df.to_excel('regular_meetings_v2.3.xlsx', sheet_name = 'Regular Meetings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_chromium()\n",
    "\n",
    "reports = []\n",
    "\n",
    "# url = f\"https://pub-ottawa.escribemeetings.com/Meeting.aspx?Id=791a957c-85f4-44b8-86a2-ce2ca3ea42c1&Agenda=Agenda&lang=English\"\n",
    "url = f\"https://pub-ottawa.escribemeetings.com/Meeting.aspx?Id=83b599bc-563c-6679-4a1e-713cfb4d00fb&Agenda=Agenda&lang=English\"\n",
    "domain = '{uri.scheme}://{uri.netloc}/'.format(uri = urlparse(url))\n",
    "driver.get(url)\n",
    "# driver.find_elements(by = \"xpath\", value = '''//*[text() = \"Chief's report\"]''')\n",
    "page_src = driver.page_source\n",
    "soup = BeautifulSoup(page_src, 'html.parser')\n",
    "\n",
    "# for a in soup.find_all('div', text = re.compile(\"Chief's report\"))\n",
    "# elems = soup.find_all('div', text = re.compile(\"Chief’s report\"))\n",
    "elems = soup.find_all(text = lambda t: t and any(x in t for x in [\"Chief’s verbal report\", \"Chief's verbal report\", \"CHIEF’S VERBAL REPORT\", \"CHIEF'S VERBAL REPORT\"]))\n",
    "for el in elems:\n",
    "    # for a in el.find_parent().find_all('a', href = True):\n",
    "    print(((el.find_parent()).find_parent()).find_parent())\n",
    "    # print(el.find_parent())\n",
    "    for a in ((el.find_parent()).find_parent()).find_parent().find_all('a', href = lambda href: href and \"filestream\" in href):\n",
    "        # if \"filestream\" in a['href']:\n",
    "            reports.append(domain + a['href'])\n",
    "print(reports)\n",
    "    # for a in ((el.find_parent()).find_parent()).find_parent().find_all('a', href = True):\n",
    "    #     print ('here')\n",
    "    #     if \"filestream\" in a['href']:\n",
    "    #         print(a['href'])\n",
    "# for x in driver.find_elements(by = \"xpath\", value = '''//*[text() = \"Chief's report\"]'''):\n",
    "#     soup = BeautifulSoup(x.parent.page_source, 'html.parser')\n",
    "#     print(soup)\n",
    "# driver.find_element(by = \"xpath\", value = '''//*[contains(text(), \"Chief's verbal report\")]''').click()\n",
    "\n",
    "# page_src = driver.page_source\n",
    "# soup = BeautifulSoup(page_src, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_mail():\n",
    "    recipient_mail = input(\"Enter an E-mail address to receive missing chief's verbal list\")\n",
    "    to_mail = recipient_mail\n",
    "    gmail_usr = os.getenv('GMAIL_USR')\n",
    "    gmail_pwd = os.getenv('GMAIL_PWD')\n",
    "\n",
    "    msg = MIMEMultipart()\n",
    "    msg['Subject'] = \"Missing Chief's Verbals List\"\n",
    "    msg['From'] = gmail_usr\n",
    "    msg['To'] = to_mail\n",
    "\n",
    "    msgText = MIMEText(\"\\n This mail consists of regular meetings list where Chief's Verbal document is missing \\n p.f.a. \\n\", 'html')\n",
    "    msg.attach(msgText)\n",
    "\n",
    "    filename = \"regular_meetings_v2.0.xlsx\"\n",
    "    xlsx = MIMEApplication(open(filename, 'rb').read())\n",
    "    xlsx.add_header('Content-Disposition', 'attachment', filename = 'missing_chief_verbals.xlsx')\n",
    "    msg.attach(xlsx)\n",
    "\n",
    "    # with open(filename, 'rb') as f:\n",
    "    #     file_data = f.read()\n",
    "    # smtpserver.add_attachment(file_data, maintype=\"application\", subtype=\"xlsx\", filename='regular_meetings.xlsx')\n",
    "\n",
    "    try:\n",
    "        with smtplib.SMTP(\"smtp.gmail.com\", 587) as smtpserver:\n",
    "            smtpserver.ehlo()\n",
    "            smtpserver.starttls()\n",
    "            smtpserver.login(gmail_usr, gmail_pwd)\n",
    "            smtpserver.sendmail(f'{gmail_usr}<Team-8 SMTP Mail Client>', to_mail, msg.as_string())\n",
    "            smtpserver.quit()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    print ('email sent!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_analysis():\n",
    "    global crimes_data_df\n",
    "    crimes_data_df = pd.DataFrame()\n",
    "    # Read the Excel file\n",
    "    excel_file = 'regular_meetings_v2.3.xlsx'\n",
    "    reports_df = pd.read_excel(excel_file)\n",
    "\n",
    "    # Assuming the column containing PDF links is named 'pdf_urls'\n",
    "    reports_df = reports_df.dropna()\n",
    "    pdf_urls = reports_df[['Date', 'Verbal Report File URL']]\n",
    "\n",
    "    print(pdf_urls)\n",
    "    # Read each PDF\n",
    "    # for pdf_url in pdf_urls:\n",
    "    for index, row in pdf_urls.iterrows():\n",
    "        resp = requests.get(row['Verbal Report File URL'])\n",
    "        with io.BytesIO(resp.content) as file:\n",
    "            # Create a PDF object\n",
    "            pdf = PyPDF2.PdfReader(file)\n",
    "            \n",
    "            # Initialize a variable to store the extracted text\n",
    "            global corpus\n",
    "            corpus = \"\"\n",
    "            \n",
    "            # Extract the text from each page of the PDF. We have only one page.\n",
    "            for page in pdf.pages:\n",
    "                corpus += page.extract_text()\n",
    "            \n",
    "        print(\"link started: \", index, row['Verbal Report File URL'])\n",
    "        sentiment_analyzer(corpus, row['Date'])\n",
    "        crimes_data_df = crimes_data_df.append(crime_df)\n",
    "        print(\"link over: \", index, row['Verbal Report File URL'])\n",
    "        print(crimes_data_df)\n",
    "        # Print the extracted text\n",
    "        # print(corpus)\n",
    "    export_excel()\n",
    "    print(\"Crime Analysis Completed! Please check the output file: 'crime_sentiment_analysis.xlsx' for the output.\")\n",
    " \n",
    "def sentiment_analyzer(corpus, date):\n",
    "    corpus = corpus.replace(\"\\n\", \" \")\n",
    "    corpus = corpus.lower()\n",
    "    tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
    "    sentences = tokenizer.tokenize(corpus)\n",
    "    # print(sentences)\n",
    "\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    crime = []\n",
    "    not_crime = []\n",
    "\n",
    "    # Loop through the sentences and determine the sentiment score\n",
    "    for sentence in sentences:\n",
    "        # Get the sentiment score for the sentence\n",
    "        sentiment_score = analyzer.polarity_scores(sentence)\n",
    "        \n",
    "        # Determine if the sentence has a negative sentiment (indicating a crime)\n",
    "        if sentiment_score['neg'] > sentiment_score['pos']:\n",
    "            crime.append(sentence)\n",
    "        else:\n",
    "            not_crime.append(sentence)\n",
    "    # print(crime)\n",
    "    global crime_df\n",
    "    crime_df = pd.DataFrame({'sentences': crime})\n",
    "    crime_df.insert(0, 'Date', date)\n",
    "\n",
    "    # apply the function to the DataFrame for contains_crime()\n",
    "    crime_df['contains_crime'] = crime_df['sentences'].apply(contains_crime)\n",
    "    crime_df = crime_df[crime_df['contains_crime'] == True]\n",
    "    print(crime_df)\n",
    "\n",
    "    #crime_df[\"num_people\"] = crime_df[\"sentences\"].apply(detect_num_people).replace(np.nan, 1).astype(int) for detect_num_people() and detect_crime_type()\n",
    "    crime_df.loc[:, \"num_people_involved\"] = crime_df[\"sentences\"].apply(detect_num_people).replace(np.nan, 1).astype(int)\n",
    "    crime_df.loc[:, \"crime_type\"] = crime_df[\"sentences\"].apply(detect_crime_type)\n",
    "\n",
    "    # Add a new column to the DataFrame with the detected crime type\n",
    "    #crime_df.loc[:, \"crime_type\"] = crime_df[\"sentences\"].apply(detect_crime_type)\n",
    "\n",
    "\n",
    "# function to detect if a sentence contains a crime\n",
    "def contains_crime(sentence):\n",
    "    # load the pre-trained model\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    # nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # example DataFrame\n",
    "    # crime_df = pd.DataFrame(crime_df)\n",
    "\n",
    "    # apply NER to the sentence\n",
    "    crime_words = ['homicide', 'murder', 'kill', 'sexual', 'assault', 'drug', 'shotgun', 'rob', 'criminal', 'charge', 'rape', 'violence', 'attack', 'sexual assault', 'robbery', 'shoot', 'gun']\n",
    "    # crime_tokens = [nlp(word) for word in crime_words]\n",
    "    # crime_vectors = np.vstack([token.vector for token in crime_tokens])\n",
    "    doc = nlp(sentence)\n",
    "    # check for entities labelled as \"CRIME\" or \"LAW\"\n",
    "    for ent in doc.ents:\n",
    "        \n",
    "        if ent.label_ in ['CRIME', 'LAW', 'MURDER', 'PERSON', 'WEAPON', 'MONEY', 'GUN', 'CRIMINAL CHARGES', 'NUMBERS']:\n",
    "            return True\n",
    "    # check for POS tags indicating a violent crime\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'VERB' and token.lemma_ in crime_words:\n",
    "            return True\n",
    "    for token in doc:\n",
    "        for crime_word in crime_words:\n",
    "            if token.similarity(nlp(crime_word)) > 0.6:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    # similarities = np.dot(crime_vectors, doc.vector.T) / (np.linalg.norm(crime_vectors) * np.linalg.norm(doc.vector))\n",
    "    # if np.any(similarities > 0.6):\n",
    "    #     return True\n",
    "    # return False\n",
    "\n",
    "\n",
    "# Define a function to detect the type of crime in a sentence\n",
    "def detect_crime_type(sentence):\n",
    "    pd.options.mode.chained_assignment = None\n",
    "    homicide_regex = re.compile(r\"(murder|killing|death|homicide|manslaughter|Guns|shooting|died|offenders|firearm|shots|fired|shoot)\", re.IGNORECASE)\n",
    "    stabbing_regex = re.compile(r\"(stabbing|stabbed|knife attack|knife)\", re.IGNORECASE)\n",
    "    police_regex = re.compile(r\"(assaulted the officer)\", re.IGNORECASE)\n",
    "    stolen_regex = re.compile(r\"(Theft|stolen|fraud|possesion)\", re.IGNORECASE)\n",
    "    drug_regex = re.compile(r\"(drug|marijuana|substances|cannabis|products|narcotics|overdosing|overdosed)\", re.IGNORECASE)\n",
    "    driving_regex = re.compile(r\"(stunt|driving|licence|demerit|fined|car)\", re.IGNORECASE)\n",
    "    hateful_regex = re.compile(r\"(hateful|hate|speech)\", re.IGNORECASE)\n",
    "    assault_regex = re.compile(r\"(sexual|sexual assault|harassment|harassing|abusing|abuse|threatening|fighting|rape)\", re.IGNORECASE)\n",
    "\n",
    "\n",
    "    if re.search(homicide_regex, sentence):\n",
    "        return \"homicide\"\n",
    "    elif re.search(stabbing_regex, sentence):\n",
    "        return \"stabbing\"\n",
    "    elif re.search(police_regex, sentence):\n",
    "        return \"total assaults against a peace officer\"\n",
    "    elif re.search(stolen_regex, sentence):\n",
    "        return \"total possession of stolen property\"\n",
    "    elif re.search(drug_regex, sentence):\n",
    "        return \"drug violations\"\n",
    "    elif re.search(driving_regex, sentence):\n",
    "        return \"driving violations\"\n",
    "    elif re.search(hateful_regex, sentence):\n",
    "        return \"speech violations\"\n",
    "    elif re.search(assault_regex, sentence):\n",
    "        return \"assault and harrassment\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "def detect_num_people(text):\n",
    "    pattern = r\"\\b(\\d+|one|two|three|four|five|six|seven|eight|nine|ten)\\b(?=\\s*(?:for\\s)?(?:criminals?|illicit?|robberies?|arrests?|suspects?|offenders?|men|people|individuals|stunt|criminal|charges))\"\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        num_str = match.group(1)\n",
    "        if num_str.isdigit():\n",
    "            num = int(num_str)\n",
    "            if num > 100: # Assume it's an age\n",
    "                return None\n",
    "            elif num >= 10 and 'old' in text: # Assume it's an age\n",
    "                return None\n",
    "            else:\n",
    "                return num\n",
    "        elif num_str == 'one':\n",
    "            return 1\n",
    "        elif num_str == 'two':\n",
    "            return 2\n",
    "        elif num_str == 'three':\n",
    "            return 3\n",
    "        elif num_str == 'four':\n",
    "            return 4\n",
    "        elif num_str == 'five':\n",
    "            return 5\n",
    "        elif num_str == 'six':\n",
    "            return 6\n",
    "        elif num_str == 'seven':\n",
    "            return 7\n",
    "        elif num_str == 'eight':\n",
    "            return 8\n",
    "        elif num_str == 'nine':\n",
    "            return 9\n",
    "        elif num_str == 'ten':\n",
    "            return 10\n",
    "    else:\n",
    "        return None\n",
    "      \n",
    "\n",
    "# Print the modified DataFrame\n",
    "def export_excel():\n",
    "    print(crimes_data_df)\n",
    "    output_path = 'crime_sentiment_analysis.xlsx'\n",
    "    crimes_data_df.to_excel(output_path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize_chromium()\n",
    "# meetings_extraction()\n",
    "# release_chromium()\n",
    "# send_mail()\n",
    "initialize_analysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "1bad07944f3d7920347c6d0b92012e122b73ccfed47649f1fe4ad4ea27f4d731"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
